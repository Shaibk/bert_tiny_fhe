import torch
import numpy as np
import time
import os
import sys
from transformers import AutoTokenizer
import desilofhe as fhe

# ÂºïÂÖ•È°πÁõÆÊ®°Âùó
sys.path.append(os.path.join(os.path.dirname(__file__), ".."))
from experiments.accuracy_first.plaintext.model_plain_tinybert import PlainTinyBert
from src.bert_layers import FHEBertTinyEncoder
from src.block_matrix import BlockMatrix

# Ê®°Êãü‰∏Ä‰∫õÊµãËØïÂè•Â≠ê (Âæ™ÁéØÂ°´ÂÖÖÂà∞ 128 ‰∏™)
TEST_SENTENCES = [
    "freeze my account", "tell me the weather", "what is your name", 
    "book a table for two", "transfer money to mom", "how is the traffic",
    "play some music", "set an alarm for 8am", "what is the date today",
    "where is the nearest gas station", "cancel my reservation", "who are you",
    "exchange rate for euro", "my card is lost", "do you like pizza",
    "what is my balance"
]

def get_batch_data(batch_size=128):
    """ÁîüÊàê 128 ‰∏™ËæìÂÖ•Âè•Â≠êÂíåÂØπÂ∫îÁöÑ Embedding"""
    sentences = [TEST_SENTENCES[i % len(TEST_SENTENCES)] for i in range(batch_size)]
    model_id = "google/bert_uncased_L-2_H-128_A-2"
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    inputs = tokenizer(sentences, return_tensors="pt", padding="max_length", max_length=32, truncation=True)
    return sentences, inputs

def load_pytorch_model(device="cpu"):
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    model_path = os.path.join(project_root, "experiments/accuracy_first/plaintext/student_8level.pt")
    
    model = PlainTinyBert(
        vocab_size=30522, max_len=32, hidden=128, layers=2, heads=2, 
        intermediate=512, dropout=0.0, 
        attn_type="2quad", attn_kwargs={"c": 4.0}, 
        act="gelu_poly_learnable", act_kwargs={"init_a": 0.02, "init_b": 0.5, "init_d": 0.5},
        norm_type="bias_only", learnable_tau=True, num_classes=150
    )
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()
    return model

def main():
    print("==========================================================")
    print("   PyTorch vs FHE: The Ultimate Agreement Test")
    print("==========================================================")
    
    # 1. ÂáÜÂ§áÊï∞ÊçÆ
    BATCH_SIZE = 128
    print(f"1. Generating {BATCH_SIZE} inputs...")
    sentences, inputs_pt = get_batch_data(BATCH_SIZE)
    input_ids_pt = inputs_pt["input_ids"]
    attention_mask_pt = inputs_pt["attention_mask"] # [ÂÖ≥ÈîÆ] Ëé∑Âèñ mask
    
    # 2. PyTorch Êé®ÁêÜ (Ground Truth)
    print("2. Running PyTorch Inference...")
    TOTAL_DAMPING = 1e-6 # Ê®°Êãü FHE ÈòªÂ∞ºÊïàÊûú
    pt_model = load_pytorch_model()
    with torch.no_grad():
        # [ÂÖ≥ÈîÆ‰øÆÂ§ç] ‰º†ÂÖ• attention_maskÔºåÈÅøÂÖç PAD Âπ≤Êâ∞È¢ÑÊµãÁªìÊûú
        pt_out = pt_model(input_ids_pt, attention_mask=attention_mask_pt)
        pt_logits = pt_out["logits"].numpy() * TOTAL_DAMPING 
        pt_preds = np.argmax(pt_logits, axis=1)
    
    print(f"   PyTorch completed. Predictions preview: {pt_preds[:10]}")

    # 3. FHE Êé®ÁêÜ
    print("\n3. Running FHE Inference (Full Batch)...")
    
    # FHE ÈÖçÁΩÆ
    NUM_HEADS = 2
    PHYSICAL_BATCH = BATCH_SIZE * NUM_HEADS 
    weights_path = "fhe_weights_8level_damped.npz" # Áî®ÈòªÂ∞ºÂêéÁöÑÊùÉÈáç
    
    # ËÆ°ÁÆó Embedding (Client Side)
    # [Ê®°Êãü] FHE ÊöÇÊó∂Êó†Ê≥ïÈ´òÊïàÂ§ÑÁêÜ MaskÔºåÊàë‰ª¨ËøôÈáåÂè™È™åËØÅÊï∞ÂÄºËÆ°ÁÆóÁöÑ‰∏ÄËá¥ÊÄß„ÄÇ
    # ‰∏∫‰∫Ü‰øùËØÅÂØπÊØîÂÖ¨Âπ≥ÔºåÊàë‰ª¨ÈúÄË¶ÅÁî® PyTorch ÁÆóÂá∫ÁöÑÊó† Mask Âπ≤Êâ∞ÁöÑ‰∏≠Èó¥ Embedding ‰Ωú‰∏∫ËæìÂÖ•Ôºü
    # ‰∏çÔºåFHE Êé®ÁêÜÁõÆÂâçÊ≤°Êúâ Mask ÈÄªËæëÔºåÊâÄ‰ª•ÂÆÉÂÆûÈôÖ‰∏äË∑ëÁöÑÊòØ "Êó† Mask" ÁöÑÁâàÊú¨„ÄÇ
    # ‰∏∫‰∫ÜËÆ© PyTorch Âíå FHE ÂØπÈΩêÔºåÊàë‰ª¨Êúâ‰∏§ÁßçÈÄâÊã©Ôºö
    # A. Áªô FHE Âä†‰∏ä Mask (ÂæàÈöæ)
    # B. ËÆ© PyTorch ‰πü‰∏çÁî® Mask (ÁÆÄÂçïÔºå‰ΩÜÈ¢ÑÊµãÁªìÊûúÂèØËÉΩÂÖ®ÊòØ 9)
    # 
    # Êó¢ÁÑ∂‰πãÂâçÁöÑÊµãËØïÊòæÁ§∫‰∏çÂä† Mask ‰ºöÂØºËá¥ÂÖ® 9ÔºåËÄåÂä†‰∏ä Mask ÂêéÈ¢ÑÊµãÊ≠£Â∏∏Ôºå
    # ËØ¥Êòé PyTorch Ê®°ÂûãÊú¨Ë∫´Ê≤°ÈóÆÈ¢ò„ÄÇ
    # Áé∞Âú®ÁöÑÁõÆÊ†áÊòØÈ™åËØÅ "FHE ÊòØÂê¶Ê≠£Á°ÆÊâßË°å‰∫ÜËÆ°ÁÆó"„ÄÇ
    # ÊâÄ‰ª•Êàë‰ª¨Â∫îËØ•ËÆ© PyTorch *‰πü‰∏çÂä† Mask*ÔºåÁúãÁúã FHE ÁöÑÁªìÊûúÊòØÂê¶ÂíåËøô‰∏™ "ÂÖ® 9" ÁöÑÁªìÊûú‰∏ÄËá¥„ÄÇ
    # Â¶ÇÊûú‰∏ÄËá¥ÔºåËØ¥Êòé FHE Ê≤°ÁÆóÈîôÔºåÂè™ÊòØÁº∫‰∫Ü Mask ÂäüËÉΩ„ÄÇ
    # 
    # [ÂÜ≥ÂÆö] ËøôÈáåÊàë‰ª¨ÊöÇÊó∂‰∏çÁªô PyTorch Âä† MaskÔºå‰ª•È™åËØÅ FHE ÁöÑËÆ°ÁÆóÊ≠£Á°ÆÊÄß (Agreement)„ÄÇ
    # Â¶ÇÊûú‰Ω†ÊÉ≥Áúã FHE ÁöÑÁúüÂÆûÈ¢ÑÊµãËÉΩÂäõÔºåÊú™Êù•ÈúÄË¶ÅÂú® bert_layers.py ÈáåÂÆûÁé∞ Mask„ÄÇ
    
    # ÂõûÈÄÄÂà∞Êó† Mask Êé®ÁêÜ‰ª•ÂØπÈΩê FHE Áé∞Áä∂
    with torch.no_grad():
        # ËøôÈáåÊïÖÊÑè‰∏ç‰º† maskÔºåÊ®°Êãü FHE ÁõÆÂâçÁöÑÁä∂ÊÄÅ
        pt_out_nomask = pt_model(input_ids_pt) 
        pt_logits_nomask = pt_out_nomask["logits"].numpy() * TOTAL_DAMPING
        pt_preds_nomask = np.argmax(pt_logits_nomask, axis=1)
        
    print(f"   PyTorch (No Mask) Preview: {pt_preds_nomask[:10]}")

    with torch.no_grad():
        x_emb = pt_model.embedding(input_ids_pt) + pt_model.pos_embedding[:, :32, :]
        x_emb = pt_model.emb_norm(x_emb) 
        x_plain_np = x_emb.numpy().astype(np.float32) 

    # ÂàùÂßãÂåñ FHE
    print("   Initializing Engine...")
    engine = fhe.GLEngine(shape=(PHYSICAL_BATCH, 64, 64), mode='gpu')
    sk = engine.create_secret_key()
    mult_key = engine.create_matrix_multiplication_key(sk)
    hadamard_key = engine.create_hadamard_multiplication_key(sk)
    transposition_key = engine.create_transposition_key(sk)
    
    # Âä†ÂØÜ
    print("   Encrypting...")
    x_packed = np.tile(x_plain_np, (NUM_HEADS, 1, 1)) 
    input_enc = BlockMatrix.encrypt_inputs(engine, x_packed, sk, block_size=64)
    
    # Âä†ËΩΩ FHE Â±Ç
    bert_l0 = FHEBertTinyEncoder(engine, mult_key, hadamard_key, transposition_key, weights_path, layer_idx=0)
    bert_l1 = FHEBertTinyEncoder(engine, mult_key, hadamard_key, transposition_key, weights_path, layer_idx=1)
    
    # ÊâßË°åÂ±Ç
    print("   Executing Layer 0...")
    out_l0 = bert_l0.forward_one_layer(input_enc)
    
    print("   Executing Layer 1...")
    out_l1 = bert_l1.forward_one_layer(out_l0)
    
    # Ëß£ÂØÜ‰∏éÊèêÂèñ
    print("\n4. Decrypting & Comparing...")
    w_data = np.load(weights_path)
    w_cls = w_data["classifier.weight"]
    b_cls = w_data["classifier.bias"]
    
    correct_count = 0
    decrypted_full = np.zeros(x_packed.shape, dtype=np.float32)
    
    for r in range(out_l1.r_grid):
        for c in range(out_l1.c_grid):
            blk = out_l1.blocks[r][c]
            if blk is not None:
                blk_np = engine.decrypt(blk, sk)
                r0, r1 = r*64, (r+1)*64
                c0, c1 = c*64, (c+1)*64
                real_r = min(32, r1) - r0
                real_c = min(128, c1) - c0
                if real_r > 0 and real_c > 0:
                    decrypted_full[:, r0:r0+real_r, c0:c0+real_c] = blk_np[:, :real_r, :real_c]

    print(f"\n{'idx':<5} | {'Sentence':<25} | {'PT(NoMask)':<10} | {'FHE ID':<8} | {'Match?':<10}")
    print("-" * 75)
    
    for i in range(BATCH_SIZE):
        cls_vec = decrypted_full[i, 0, :] 
        fhe_logits = np.dot(cls_vec, w_cls.T) + b_cls
        fhe_pred = np.argmax(fhe_logits)
        
        # ÂØπÊØîÂØπË±°ÊòØÊó† Mask ÁöÑ PyTorch ÁªìÊûú
        pt_target = pt_preds_nomask[i]
        
        is_match = (fhe_pred == pt_target)
        if is_match: correct_count += 1
        
        if i < 10 or i == BATCH_SIZE - 1:
            short_sent = (sentences[i][:22] + '..') if len(sentences[i]) > 22 else sentences[i]
            match_str = "‚úÖ" if is_match else "‚ùå"
            print(f"{i:<5} | {short_sent:<25} | {pt_target:<10} | {fhe_pred:<8} | {match_str:<10}")

    agreement_rate = (correct_count / BATCH_SIZE) * 100
    print("-" * 75)
    print(f"üèÜ Final Agreement Rate: {agreement_rate:.2f}% ({correct_count}/{BATCH_SIZE})")
    
    if agreement_rate > 99.0:
        print("\nüéâ SUCCESS: FHE matches Plaintext (No Mask) perfectly!")
    else:
        print("\n‚ö†Ô∏è Warning: Still mismatching. Check Damping or Calculation logic.")

if __name__ == "__main__":
    main()